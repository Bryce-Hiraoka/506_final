AI-Generated Text Detection - Project Proposal
Bryce Hiraoka

Description of the Project:
The goal of this project is to develop machine learning (ML) models that can accurately classify whether a text is generated by a human or by a large language model (LLM). Using logistic regression, support vector machines, and other feature engineering techniques, we aim to explore the limits of current models in distinguishing AI-generated text from human-authored content. We will evaluate model accuracy and performance, seeking to answer how well an ML model can detect AI-generated text across diverse datasets.

Clear Goals:
Successfully classify texts as human-generated or AI-generated.
Evaluate the accuracy and robustness of different ML models in this classification task.
Investigate the impact of feature engineering and dataset balancing on model performance.
Data Collection:
We will leverage publicly available datasets to train and test the models, focusing on the following:

AI-Text-Detection-Pile (Hugging Face): 1.418m samples (990k human, 340k AI).
LLM-Detect-AI-Generated-Text (Kaggle): 27k samples (17k human, 11k AI).
PaLm-Generated-Essays (Kaggle): 1.3k AI-generated essays.
Combined-Set (Kaggle): 87k samples (55k human, 32k AI).
AI-vs-Human-Text (Kaggle): 500k samples (305k human, 195k AI).
Human-vs-LLM-Corpus (Kaggle): 800k samples (360k human, 440k AI).
Argu-GPT (Kaggle): 7k AI-generated argumentative texts.
These datasets have a combined distribution of 1.73m human samples and 1m AI samples. We will balance the data through oversampling or undersampling techniques when necessary.

Data Modeling:
We plan to experiment with several ML algorithms:

Logistic Regression: A simple yet effective baseline model for binary classification.
Support Vector Machines (SVM): For identifying complex decision boundaries in the feature space.
Text Feature Engineering: We will focus on converting raw text into a structured, learnable format through techniques like TF-IDF, word embeddings (Word2Vec, GloVe), and tokenization.
Dataset Balancing: Applying oversampling/undersampling to mitigate the effects of imbalanced data.
Neural Networks (optional): We may explore deep learning methods (e.g., a simple LSTM) if traditional models don’t provide satisfactory results.
Data Visualization:
To gain insights into the data and model performance, we plan to use:

t-SNE plots: For visualizing high-dimensional text embeddings in 2D space.
Confusion matrices: To display the performance of the models across the human and AI classifications.
Feature importance plots: To understand which features are most useful in distinguishing between human and AI text.
ROC curves: To evaluate model performance and compare the trade-offs between true positive and false positive rates.
Test Plan:
We plan to split the datasets into training, validation, and test sets, using an 80/20 split for training and testing. In addition:

Cross-validation: To ensure model robustness, we will use k-fold cross-validation on the training data.
Temporal testing: We will train the models on data collected from older language models and test on more recent models (e.g., GPT-4 versus GPT-3), to evaluate the model’s adaptability to newer models.
Out-of-distribution testing: The model will also be tested on datasets from different domains (e.g., formal writing versus creative writing) to ensure generalization across text types.
