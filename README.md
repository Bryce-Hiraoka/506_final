AI-Generated Text Detection 
Bryce Hiraoka

My full final report is here: <a href="https://github.com/Bryce-Hiraoka/506_final/blob/main/NeurIPS_2023.pdf" target="_blank" rel="noopener noreferrer">FINAL PAPER</a> 


Description of the Project:
The goal of this project is to develop machine learning (ML) models that can accurately classify whether a text is generated by a human or by a large language model (LLM). Using logistic regression, support vector machines, and other feature engineering techniques, we aim to explore the limits of current models in distinguishing AI-generated text from human-authored content. We will evaluate model accuracy and performance, seeking to answer how well an ML model can detect AI-generated text across diverse datasets.

Clear Goals:
Successfully classify texts as human-generated or AI-generated.
Evaluate the accuracy and robustness of different ML models in this classification task.
Investigate the impact of feature engineering and dataset balancing on model performance.
Data Collection:
I will leverage publicly available datasets to train and test the models, focusing on the following:

Data Modeling:

Logistic Regression: This has shown the best results during preliminary testing with accuracy scores averaging in the 70s. 
Support Vector Machines (SVM): This has shown some promise so far and is still an option if mixed with the correct features.
Text Feature Engineering: I converted raw text into a structured, learnable format through techniques like TF-IDF, and tokenization.
Dataset Balancing: I applied oversampling/undersampling to mitigate the effects of imbalanced data. However, some models are still feeling the effect of over sampling.
Neural Networks (optional): I decided not to persue this.

Data Visualization:
To gain insights into the data and model performance, I plan to use:

t-SNE plots: For visualizing high-dimensional text embeddings in 2D space.
Confusion matrices: To display the performance of the models across the human and AI classifications.
Feature importance plots: To understand which features are most useful in distinguishing between human and AI text.
ROC curves: To evaluate model performance and compare the trade-offs between true positive and false positive rates.

Feature update:
Some promising features found through testing are number of uppercase words, number of parts of speech used, and readablilty. 
Currently, I am using the Coleman Liau score to test for readability. This has the advantage of being the most accurate but the disadvantage of taking a long time.


Test Plan:
We plan to split the datasets into training, validation, and test sets, using an 80/20 split for training and testing. In addition:

Cross-validation: This has yet to be implemented and will be worked on once I have a clearer view of our features and models
Temporal testing: I have decided to try and switch our datasets to reflect only 1 LLM (gpt3.0) to get more accurate results.
Out-of-distribution testing: I have not done this yet as I have not chosen a model or features
